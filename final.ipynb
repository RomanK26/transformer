{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7635967,"sourceType":"datasetVersion","datasetId":4449763},{"sourceId":7640342,"sourceType":"datasetVersion","datasetId":4452857}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# install required dependencies**\n","metadata":{"_uuid":"4deaf085-ffb6-4b9b-8632-8e5a37da473b","_cell_guid":"1e93e4b4-c5e1-43d6-b853-8bfdf1f6d972","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install tqdm spacy ","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:23:10.503086Z","iopub.execute_input":"2024-02-17T03:23:10.503865Z","iopub.status.idle":"2024-02-17T03:23:24.779579Z","shell.execute_reply.started":"2024-02-17T03:23:10.503833Z","shell.execute_reply":"2024-02-17T03:23:24.778493Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\nRequirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (3.7.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (6.4.0)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.3.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.24.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install indic-nlp-library","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:23:37.330886Z","iopub.execute_input":"2024-02-17T03:23:37.331668Z","iopub.status.idle":"2024-02-17T03:23:53.371905Z","shell.execute_reply.started":"2024-02-17T03:23:37.331630Z","shell.execute_reply":"2024-02-17T03:23:53.370950Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting indic-nlp-library\n  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\nCollecting sphinx-argparse (from indic-nlp-library)\n  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\nRequirement already satisfied: sphinx-rtd-theme in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (0.2.4)\nCollecting morfessor (from indic-nlp-library)\n  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (2.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (1.24.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2023.4)\nCollecting sphinx>=1.2.0 (from sphinx-argparse->indic-nlp-library)\n  Downloading sphinx-7.2.6-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\nCollecting sphinxcontrib-applehelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-devhelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-jsmath (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\nCollecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl.metadata (2.4 kB)\nCollecting sphinxcontrib-qthelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\nRequirement already satisfied: Pygments>=2.14 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.17.2)\nRequirement already satisfied: docutils<0.21,>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.20.1)\nRequirement already satisfied: snowballstemmer>=2.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\nRequirement already satisfied: babel>=2.9 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\nCollecting alabaster<0.8,>=0.7 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\nCollecting imagesize>=1.3 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\nRequirement already satisfied: requests>=2.25.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\nRequirement already satisfied: packaging>=21.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2023.11.17)\nDownloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinx-7.2.6-py3-none-any.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading alabaster-0.7.16-py3-none-any.whl (13 kB)\nDownloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl (92 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl (120 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl (83 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: morfessor, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, imagesize, alabaster, sphinx, sphinx-argparse, indic-nlp-library\nSuccessfully installed alabaster-0.7.16 imagesize-1.4.1 indic-nlp-library-0.92 morfessor-2.0.6 sphinx-7.2.6 sphinx-argparse-0.4.0 sphinxcontrib-applehelp-1.0.8 sphinxcontrib-devhelp-1.0.6 sphinxcontrib-htmlhelp-2.0.5 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.7 sphinxcontrib-serializinghtml-1.1.10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# IMPORT REQUIRED LIBRARIES","metadata":{}},{"cell_type":"code","source":"import math\nimport random\nimport time\nfrom torch import nn, optim\nfrom torch.optim import Adam\nimport torch\nfrom tqdm import tqdm \nimport spacy\nimport re\nimport os\nfrom indicnlp.tokenize import indic_tokenize\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import Counter\nfrom torch.utils.data import Dataset,DataLoader\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    print(\"Runnign on GPU\")\nelse:\n    print(\"No GPU\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:26:13.125243Z","iopub.execute_input":"2024-02-17T03:26:13.125640Z","iopub.status.idle":"2024-02-17T03:26:13.132922Z","shell.execute_reply.started":"2024-02-17T03:26:13.125614Z","shell.execute_reply":"2024-02-17T03:26:13.132031Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Runnign on GPU\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EMBEDDING PART","metadata":{}},{"cell_type":"code","source":"class TokenEmbedding(nn.Embedding):\n    \"\"\"\n    Token Embedding using torch.nn\n    they will dense representation of word using weighted matrix\n    \"\"\"\n\n    def __init__(self, vocab_size, d_model):\n        \"\"\"\n        class for token embedding that included positional information\n\n        :param vocab_size: size of vocabulary\n        :param d_model: dimensions of model\n        \"\"\"\n        super().__init__(vocab_size, d_model, padding_idx=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:26:17.481004Z","iopub.execute_input":"2024-02-17T03:26:17.481887Z","iopub.status.idle":"2024-02-17T03:26:17.487195Z","shell.execute_reply.started":"2024-02-17T03:26:17.481855Z","shell.execute_reply":"2024-02-17T03:26:17.486347Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \"\"\"\n    compute sinusoid encoding.\n    \"\"\"\n\n    def __init__(self, d_model, max_len, device):\n        \"\"\"\n        constructor of sinusoid encoding class\n\n        :param d_model: dimension of model\n        :param max_len: max sequence length\n        :param device: hardware device setting\n        \"\"\"\n        super(PositionalEncoding, self).__init__()\n\n        # same size with input matrix (for adding with input matrix)\n        self.encoding = torch.zeros(max_len, d_model, device=device)\n        self.encoding.requires_grad = False  # we don't need to compute gradient\n\n        pos = torch.arange(0, max_len, device=device)\n        pos = pos.float().unsqueeze(dim=1)\n        # 1D => 2D unsqueeze to represent word's position\n\n        _2i = torch.arange(0, d_model, step=2, device=device).float()\n        # 'i' means index of d_model (e.g. embedding size = 50, 'i' = [0,50])\n        # \"step=2\" means 'i' multiplied with two (same with 2 * i)\n\n        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n        # compute positional encoding to consider positional information of words\n\n    def forward(self, x):\n        # self.encoding\n        # [max_len = 512, d_model = 512]\n\n        batch_size, seq_len = x.size()\n        # [batch_size = 128, seq_len = 30]\n\n        return self.encoding[:seq_len, :]\n        # [seq_len = 30, d_model = 512]\n        # it will add with tok_emb : [128, 30, 512]\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:26:19.011427Z","iopub.execute_input":"2024-02-17T03:26:19.011784Z","iopub.status.idle":"2024-02-17T03:26:19.020781Z","shell.execute_reply.started":"2024-02-17T03:26:19.011758Z","shell.execute_reply":"2024-02-17T03:26:19.019858Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class TransformerEmbedding(nn.Module):\n    \"\"\"\n    token embedding + positional encoding (sinusoid)\n    positional encoding can give positional information to network\n    \"\"\"\n\n    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n        \"\"\"\n        class for word embedding that included positional information\n\n        :param vocab_size: size of vocabulary\n        :param d_model: dimensions of model\n        \"\"\"\n        \n        super(TransformerEmbedding, self).__init__()\n        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n        self.pos_emb = PositionalEncoding(d_model, max_len, device)\n        self.drop_out = nn.Dropout(p=drop_prob)\n        self.device=device\n        \n\n    def forward(self, x):\n        x = x.long().to(self.device)\n        tok_emb = self.tok_emb(x)\n        pos_emb = self.pos_emb(x)\n        return self.drop_out(tok_emb + pos_emb)","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:26:20.495366Z","iopub.execute_input":"2024-02-17T03:26:20.496166Z","iopub.status.idle":"2024-02-17T03:26:20.503062Z","shell.execute_reply.started":"2024-02-17T03:26:20.496135Z","shell.execute_reply":"2024-02-17T03:26:20.502175Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# MODEL BLOCKS","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n\n    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n        super().__init__()\n        self.emb = TransformerEmbedding(d_model=d_model,\n                                        max_len=max_len,\n                                        vocab_size=enc_voc_size,\n                                        drop_prob=drop_prob,\n                                        device=device)\n\n        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n                                                  ffn_hidden=ffn_hidden,\n                                                  n_head=n_head,\n                                                  drop_prob=drop_prob)\n                                     for _ in range(n_layers)])\n\n    def forward(self, x, src_mask):\n        x = self.emb(x)\n\n        for layer in self.layers:\n            x = layer(x, src_mask)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:26:22.330715Z","iopub.execute_input":"2024-02-17T03:26:22.331669Z","iopub.status.idle":"2024-02-17T03:26:22.338870Z","shell.execute_reply.started":"2024-02-17T03:26:22.331637Z","shell.execute_reply":"2024-02-17T03:26:22.337799Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n        super().__init__()\n        self.emb = TransformerEmbedding(d_model=d_model,\n                                        drop_prob=drop_prob,\n                                        max_len=max_len,\n                                        vocab_size=dec_voc_size,\n                                        device=device)\n\n        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n                                                  ffn_hidden=ffn_hidden,\n                                                  n_head=n_head,\n                                                  drop_prob=drop_prob)\n                                     for _ in range(n_layers)])\n\n        self.linear = nn.Linear(d_model, dec_voc_size)\n\n    def forward(self, trg, enc_src, trg_mask, src_mask):\n        trg = self.emb(trg)\n\n        for layer in self.layers:\n            trg = layer(trg, enc_src, trg_mask, src_mask)\n\n        # pass to LM head\n        output = self.linear(trg)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:26:23.452704Z","iopub.execute_input":"2024-02-17T03:26:23.453131Z","iopub.status.idle":"2024-02-17T03:26:23.461170Z","shell.execute_reply.started":"2024-02-17T03:26:23.453096Z","shell.execute_reply":"2024-02-17T03:26:23.460224Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n\n    def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,\n                 ffn_hidden, n_layers, drop_prob, device):\n        super().__init__()\n        self.src_pad_idx = src_pad_idx\n        self.trg_pad_idx = trg_pad_idx\n        self.trg_sos_idx = trg_sos_idx\n        self.device = device\n        self.encoder = Encoder(d_model=d_model,\n                               n_head=n_head,\n                               max_len=max_len,\n                               ffn_hidden=ffn_hidden,\n                               enc_voc_size=enc_voc_size,\n                               drop_prob=drop_prob,\n                               n_layers=n_layers,\n                               device=device)\n\n        self.decoder = Decoder(d_model=d_model,\n                               n_head=n_head,\n                               max_len=max_len,\n                               ffn_hidden=ffn_hidden,\n                               dec_voc_size=dec_voc_size,\n                               drop_prob=drop_prob,\n                               n_layers=n_layers,\n                               device=device)\n\n    def forward(self, src, trg):\n        src_mask = self.make_src_mask(src)\n        trg_mask = self.make_trg_mask(trg)\n        enc_src = self.encoder(src, src_mask)\n        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n        return output\n\n    def make_src_mask(self, src):\n        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n        return src_mask\n\n    def make_trg_mask(self, trg):\n        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n        trg_len = trg.shape[1]\n        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len)).type(torch.ByteTensor).to(trg.device)\n        trg_mask = trg_pad_mask & trg_sub_mask\n        return trg_mask","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:26:24.518529Z","iopub.execute_input":"2024-02-17T03:26:24.519237Z","iopub.status.idle":"2024-02-17T03:26:24.529924Z","shell.execute_reply.started":"2024-02-17T03:26:24.519204Z","shell.execute_reply":"2024-02-17T03:26:24.528986Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class ScaleDotProductAttention(nn.Module):\n    \"\"\"\n    compute scale dot product attention\n\n    Query : given sentence that we focused on (decoder)\n    Key : every sentence to check relationship with Qeury(encoder)\n    Value : every sentence same with Key (encoder)\n    \"\"\"\n\n    def __init__(self):\n        super(ScaleDotProductAttention, self).__init__()\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, q, k, v, mask=None, e=1e-12):\n        # input is 4 dimension tensor\n        # [batch_size, head, length, d_tensor]\n        batch_size, head, length, d_tensor = k.size()\n\n        # 1. dot product Query with Key^T to compute similarity\n        k_t = k.transpose(2, 3)  # transpose\n        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n\n        # 2. apply masking (opt)\n        if mask is not None:\n            mask=mask.to(q.device)\n            score = score.masked_fill(mask == 0, -10000)\n\n        # 3. pass them softmax to make [0, 1] range\n        score = self.softmax(score)\n\n        # 4. multiply with Value\n        v = score @ v\n\n        return v, score","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:26:25.755714Z","iopub.execute_input":"2024-02-17T03:26:25.756075Z","iopub.status.idle":"2024-02-17T03:26:25.763926Z","shell.execute_reply.started":"2024-02-17T03:26:25.756048Z","shell.execute_reply":"2024-02-17T03:26:25.762955Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n\n    def __init__(self, d_model, n_head):\n        super(MultiHeadAttention, self).__init__()\n        self.n_head = n_head\n        self.attention = ScaleDotProductAttention()\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_concat = nn.Linear(d_model, d_model)\n\n    def forward(self, q, k, v, mask=None):\n        # 1. dot product with weight matrices\n        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n\n        # 2. split tensor by number of heads\n        q, k, v = self.split(q), self.split(k), self.split(v)\n\n        # 3. do scale dot product to compute similarity\n        out, attention = self.attention(q, k, v, mask=mask)\n\n        # 4. concat and pass to linear layer\n        out = self.concat(out)\n        out = self.w_concat(out)\n\n        # 5. visualize attention map\n        # TODO : we should implement visualization\n\n        return out\n\n    def split(self, tensor):\n        \"\"\"\n        split tensor by number of head\n\n        :param tensor: [batch_size, length, d_model]\n        :return: [batch_size, head, length, d_tensor]\n        \"\"\"\n        batch_size, length, d_model = tensor.size()\n\n        d_tensor = d_model // self.n_head\n        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n        # it is similar with group convolution (split by number of heads)\n\n        return tensor\n\n    def concat(self, tensor):\n        \"\"\"\n        inverse function of self.split(tensor : torch.Tensor)\n\n        :param tensor: [batch_size, head, length, d_tensor]\n        :return: [batch_size, length, d_model]\n        \"\"\"\n        batch_size, head, length, d_tensor = tensor.size()\n        d_model = head * d_tensor\n\n        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n        return tensor\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:26:26.820077Z","iopub.execute_input":"2024-02-17T03:26:26.820422Z","iopub.status.idle":"2024-02-17T03:26:26.832313Z","shell.execute_reply.started":"2024-02-17T03:26:26.820396Z","shell.execute_reply":"2024-02-17T03:26:26.831296Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n\n    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n        super(EncoderLayer, self).__init__()\n        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n        self.norm1 = LayerNorm(d_model=d_model)\n        self.dropout1 = nn.Dropout(p=drop_prob)\n\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.norm2 = LayerNorm(d_model=d_model)\n        self.dropout2 = nn.Dropout(p=drop_prob)\n\n    def forward(self, x, src_mask):\n        # 1. compute self attention\n        _x = x\n        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n        \n        # 2. add and norm\n        x = self.dropout1(x)\n        x = self.norm1(x + _x)\n        \n        # 3. positionwise feed forward network\n        _x = x\n        x = self.ffn(x)\n      \n        # 4. add and norm\n        x = self.dropout2(x)\n        x = self.norm2(x + _x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:26:28.106100Z","iopub.execute_input":"2024-02-17T03:26:28.106600Z","iopub.status.idle":"2024-02-17T03:26:28.117553Z","shell.execute_reply.started":"2024-02-17T03:26:28.106559Z","shell.execute_reply":"2024-02-17T03:26:28.116577Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n\n    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n        super(DecoderLayer, self).__init__()\n        self.self_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n        self.norm1 = LayerNorm(d_model=d_model)\n        self.dropout1 = nn.Dropout(p=drop_prob)\n\n        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n        self.norm2 = LayerNorm(d_model=d_model)\n        self.dropout2 = nn.Dropout(p=drop_prob)\n\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.norm3 = LayerNorm(d_model=d_model)\n        self.dropout3 = nn.Dropout(p=drop_prob)\n\n    def forward(self, dec, enc, trg_mask, src_mask):\n        # 1. compute self attention\n        _x = dec\n        x = self.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n        \n        # 2. add and norm\n        x = self.dropout1(x)\n        x = self.norm1(x + _x)\n\n        if enc is not None:\n            # 3. compute encoder - decoder attention\n            _x = x\n            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=src_mask)\n            \n            # 4. add and norm\n            x = self.dropout2(x)\n            x = self.norm2(x + _x)\n\n        # 5. positionwise feed forward network\n        _x = x\n        x = self.ffn(x)\n        \n        # 6. add and norm\n        x = self.dropout3(x)\n        x = self.norm3(x + _x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:27:09.617106Z","iopub.execute_input":"2024-02-17T03:27:09.617442Z","iopub.status.idle":"2024-02-17T03:27:09.628166Z","shell.execute_reply.started":"2024-02-17T03:27:09.617417Z","shell.execute_reply":"2024-02-17T03:27:09.627321Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    def __init__(self, d_model, eps=1e-12):\n        super(LayerNorm, self).__init__()\n        self.gamma = nn.Parameter(torch.ones(d_model))\n        self.beta = nn.Parameter(torch.zeros(d_model))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        var = x.var(-1, unbiased=False, keepdim=True)\n        # '-1' means last dimension. \n\n        out = (x - mean) / torch.sqrt(var + self.eps)\n        out = self.gamma * out + self.beta\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:27:09.965122Z","iopub.execute_input":"2024-02-17T03:27:09.965994Z","iopub.status.idle":"2024-02-17T03:27:09.972688Z","shell.execute_reply.started":"2024-02-17T03:27:09.965963Z","shell.execute_reply":"2024-02-17T03:27:09.971769Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n\n    def __init__(self, d_model, hidden, drop_prob=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.linear1 = nn.Linear(d_model, hidden)\n        self.linear2 = nn.Linear(hidden, d_model)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=drop_prob)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:27:11.380876Z","iopub.execute_input":"2024-02-17T03:27:11.381556Z","iopub.status.idle":"2024-02-17T03:27:11.388023Z","shell.execute_reply.started":"2024-02-17T03:27:11.381526Z","shell.execute_reply":"2024-02-17T03:27:11.387011Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:27:12.245023Z","iopub.execute_input":"2024-02-17T03:27:12.245824Z","iopub.status.idle":"2024-02-17T03:27:12.250246Z","shell.execute_reply.started":"2024-02-17T03:27:12.245794Z","shell.execute_reply":"2024-02-17T03:27:12.249324Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def bleu_stats(hypothesis, reference):\n    \"\"\"Compute statistics for BLEU.\"\"\"\n    stats = []\n    stats.append(len(hypothesis))\n    stats.append(len(reference))\n    for n in range(1, 5):\n        s_ngrams = Counter(\n            [tuple(hypothesis[i:i + n]) for i in range(len(hypothesis) + 1 - n)]\n        )\n        r_ngrams = Counter(\n            [tuple(reference[i:i + n]) for i in range(len(reference) + 1 - n)]\n        )\n\n        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n        stats.append(max([len(hypothesis) + 1 - n, 0]))\n    return stats\n\n\ndef bleu(stats):\n    \"\"\"Compute BLEU given n-gram statistics.\"\"\"\n    if len(list(filter(lambda x: x == 0, stats))) > 0:\n        return 0\n    (c, r) = stats[:2]\n    log_bleu_prec = sum(\n        [math.log(float(x) / y) for x, y in zip(stats[2::2], stats[3::2])]\n    ) / 4.\n    return math.exp(min([0, 1 - float(r) / c]) + log_bleu_prec)\n\n\ndef get_bleu(hypotheses, reference):\n    \"\"\"Get validation BLEU score for dev set.\"\"\"\n    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n    for hyp, ref in zip(hypotheses, reference):\n        stats += np.array(bleu_stats(hyp, ref))\n    return 100 * bleu(stats)\n\n\ndef idx_to_word(x, vocab):\n    words = []\n    for i in x:\n        word = vocab.itos[i]\n        if '<' not in word:\n            words.append(word)\n    words = \" \".join(words)\n    return words\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:27:13.498915Z","iopub.execute_input":"2024-02-17T03:27:13.499614Z","iopub.status.idle":"2024-02-17T03:27:13.512404Z","shell.execute_reply.started":"2024-02-17T03:27:13.499569Z","shell.execute_reply":"2024-02-17T03:27:13.511509Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# DATA LOADING , TOKENIZATION, VOCAB_BUILDING, TOKEN_TO_INDEX","metadata":{}},{"cell_type":"code","source":"\n\ndef load_tokenizers():\n    return indic_tokenize, spacy.load('en_core_web_sm')\n\ndef tokenize_ne(text: str, tokenizer):\n        return [tok for tok in tokenizer.trivial_tokenize(text)]\n\n\n\ndef tokenize_en(text:str,tokenizer):\n    return [tok.text for tok in tokenizer.tokenizer(text)]\n\n\n\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, source: str, target: str):\n        self.nepali_root = source\n        self.english_root = target\n        self.current=0\n        self.max_src_len=0\n        self.max_trg_len=0\n        self.tokenizers =load_tokenizers()\n        self.src_vocab=set()\n        self.trg_vocab=set()\n        self.src_vocab.update(['<sos>', '<eos>', '<pad>','<unk>']) \n        self.trg_vocab.update(['<sos>', '<eos>', '<pad>','<unk>'])\n        self.total_sentences = 0\n        self.data = []\n\n        \n        with open(self.nepali_root, 'r') as nepali, open(self.english_root, 'r') as english:\n            for nep, eng in zip(nepali, english):\n                tokenized_nepali = tokenize_ne(nep, self.tokenizers[0])\n                tokenized_eng = tokenize_en(eng, self.tokenizers[1])\n                self.total_sentences += 1\n                self.max_src_len = max(self.max_src_len, len(tokenized_nepali))\n                self.max_trg_len = max(self.max_trg_len, len(tokenized_eng))\n                self.src_vocab.update(tokenized_nepali)\n                self.trg_vocab.update(tokenized_eng)\n                self.data.append((tokenized_nepali, tokenized_eng))\n                random.shuffle(self.data)\n\n        \n        self.src_vocab_dict={word: i for i, word in enumerate(self.src_vocab)}\n        self.trg_vocab_dict={word: i for i, word in enumerate(self.trg_vocab)}\n        \n\n        self.trg_pad_idx = self.trg_vocab_dict['<pad>']\n        self.trg_sos_idx = self.trg_vocab_dict['<sos>']\n        self.trg_eos_idx = self.trg_vocab_dict['<eos>']\n        self.src_pad_idx = self.src_vocab_dict['<pad>']\n        self.src_sos_idx = self.src_vocab_dict['<sos>']   \n        self.src_eos_idx = self.src_vocab_dict['<eos>']\n        self.src_unk_idx = self.src_vocab_dict['<unk>'] \n        self.trg_unk_idx = self.trg_vocab_dict['<unk>']\n        self.enc_voc_size = len(self.src_vocab)\n        self.dec_voc_size= len(self.trg_vocab)\n        \n        random.shuffle(self.data)\n        train_ratio = 0.8\n        val_ratio = 0.1\n        data_len = len(self.data)\n        train_size = int(data_len * train_ratio)\n        val_size = int(data_len * val_ratio)\n        test_size = data_len - train_size - val_size\n\n\n        self.train_data = self.data[:train_size]\n        self.val_data = self.data[train_size:train_size+val_size]\n        self.test_data = self.data[train_size+val_size:]\n  \n\n\n    def __len__(self):\n        if self.train_data:\n            return len(self.train_data)\n        elif self.val_data:\n            return len(self.val_data)\n        elif self.test_data:\n            return len(self.test_data)\n        else:\n            raise ValueError(\"No data available!\")\n\n\n    def __getitem__(self, idx):\n        src, trg = self.train_data[idx]\n        src = [self.src_vocab_dict[token] if token in self.src_vocab_dict else self.src_vocab_dict['<unk>'] for token in src]\n        trg = [self.trg_vocab_dict[token] if token in self.trg_vocab_dict else self.trg_vocab_dict['<unk>'] for token in trg]\n        return [src, trg]   \n       \n\n\n    def __iter__(self):\n        return self    \n\n\n\n    def __next__(self):\n        if self.current < len(self.train_data):\n            self.current += 1\n            return self.__getitem__(self.current)\n\n        raise StopIteration \n\n\n    def printv(self):\n        print(self.src_vocab_dict)\n\n           \n\ndef custom_collate(batch, src_sos_idx, src_eos_idx, trg_sos_idx, trg_eos_idx, src_pad_idx, trg_pad_idx,src_vocab_dict:dict,trg_vocab_dict:dict):\n    src_batch, trg_batch = zip(*batch)\n#     print(src_batch,trg_batch)\n    # print(\"Batch Sizes (Before Padding):\", [len(src) for src in src_batch], [len(trg) for trg in trg_batch])\n    src_batch = [[src_vocab_dict[token] if token in src_vocab_dict else src_vocab_dict['<unk>'] for token in src] for src in src_batch]\n    trg_batch = [[trg_vocab_dict[token] if token in trg_vocab_dict else trg_vocab_dict['<unk>'] for token in trg] for trg in trg_batch]\n    # Pad sequences to the fixed length max_len\n    padded_src = [torch.cat([torch.tensor([src_sos_idx]), torch.tensor(src), torch.tensor([src_eos_idx]), torch.full((max_len - len(src) - 2,), src_pad_idx, dtype=torch.long)]) for src in src_batch]\n    padded_trg = [torch.cat([torch.tensor([trg_sos_idx]), torch.tensor(trg), torch.tensor([trg_eos_idx]), torch.full((max_len - len(trg) - 2,), trg_pad_idx, dtype=torch.long)]) for trg in trg_batch]\n    \n    # Stack the padded sequences\n    padded_src = torch.stack(padded_src)\n    padded_trg = torch.stack(padded_trg)\n    # print(\"Batch Sizes (Before Padding):\", [len(src) for src in padded_src], [len(trg) for trg in padded_trg])\n    # print(\"Batch Sizes (After Padding):\", padded_src.shape, padded_trg.shape)\n    return [padded_src, padded_trg]\n\n         \n\n\ndataset = CustomDataset('/kaggle/input/150kdata/Dataset.ne', '/kaggle/input/150kdata/Dataset.en')","metadata":{"execution":{"iopub.status.busy":"2024-02-17T03:27:15.415343Z","iopub.execute_input":"2024-02-17T03:27:15.415818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint (dataset.max_src_len, dataset.max_trg_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\nmax_len = 45\nd_model = 512\nn_layers = 6\nn_heads = 8\nffn_hidden = 1024\ndrop_prob = 0.1\n\n# optimizer parameter setting\ninit_lr = 1e-5\nfactor = 0.9\nadam_eps = 5e-9\npatience = 10\nwarmup = 100\nepoch = 100  #1000\nclip = 1.0\nweight_decay = 5e-4\ninf = float('inf')","metadata":{"execution":{"iopub.status.busy":"2024-02-17T01:53:42.638295Z","iopub.execute_input":"2024-02-17T01:53:42.639154Z","iopub.status.idle":"2024-02-17T01:53:42.644578Z","shell.execute_reply.started":"2024-02-17T01:53:42.639122Z","shell.execute_reply":"2024-02-17T01:53:42.643573Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil","metadata":{"execution":{"iopub.status.busy":"2024-02-17T01:53:44.217134Z","iopub.execute_input":"2024-02-17T01:53:44.217808Z","iopub.status.idle":"2024-02-17T01:53:44.221703Z","shell.execute_reply.started":"2024-02-17T01:53:44.217775Z","shell.execute_reply":"2024-02-17T01:53:44.220769Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# to delete the modesl from kaggle/working except kaggle/working/result","metadata":{}},{"cell_type":"code","source":"# root_dir = \"/kaggle/working\"\n# excluded_dir = \"result\"\n\n# # Get all files and directories in the root directory\n# all_items = os.listdir(root_dir)\n\n# # Filter out the excluded directory\n# items_to_delete = [item for item in all_items if item != excluded_dir]\n\n# # Delete each item (file or directory)\n# for item in items_to_delete:\n#     item_path = os.path.join(root_dir, item)\n#     if os.path.isdir(item_path):\n#         shutil.rmtree(item_path)  # Use shutil.rmtree for directories\n#     else:\n#         os.remove(item_path)\n\n# print(f\"Successfully deleted all items except '{excluded_dir}' in {root_dir}.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-17T01:53:46.359837Z","iopub.execute_input":"2024-02-17T01:53:46.360508Z","iopub.status.idle":"2024-02-17T01:53:46.367220Z","shell.execute_reply.started":"2024-02-17T01:53:46.360475Z","shell.execute_reply":"2024-02-17T01:53:46.366370Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Successfully deleted all items except 'result' in /kaggle/working.\n","output_type":"stream"}]},{"cell_type":"code","source":"# dataset = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=lambda batch: custom_collate(batch, dataset.src_sos_idx, dataset.src_eos_idx, dataset.trg_sos_idx, dataset.trg_eos_idx, dataset.src_pad_idx, dataset.trg_pad_idx))\ntrain_iter = DataLoader(dataset.train_data, batch_size=16, shuffle=True, collate_fn=lambda batch: custom_collate(batch, dataset.src_sos_idx, dataset.src_eos_idx, dataset.trg_sos_idx, dataset.trg_eos_idx, dataset.src_pad_idx, dataset.trg_pad_idx,dataset.src_vocab_dict,dataset.trg_vocab_dict))\ntest_iter = DataLoader(dataset.test_data, batch_size=16, shuffle=False, collate_fn=lambda batch: custom_collate(batch, dataset.src_sos_idx, dataset.src_eos_idx, dataset.trg_sos_idx, dataset.trg_eos_idx, dataset.src_pad_idx, dataset.trg_pad_idx,dataset.src_vocab_dict,dataset.trg_vocab_dict))\nvalid_iter = DataLoader(dataset.val_data, batch_size=16, shuffle=False, collate_fn=lambda batch: custom_collate(batch, dataset.src_sos_idx, dataset.src_eos_idx, dataset.trg_sos_idx, dataset.trg_eos_idx, dataset.src_pad_idx, dataset.trg_pad_idx,dataset.src_vocab_dict,dataset.trg_vocab_dict))\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef initialize_weights(m):\n    if hasattr(m, 'weight') and m.weight.dim() > 1:\n        nn.init.kaiming_uniform_(m.weight.data)\n\n\nmodel = Transformer(src_pad_idx=dataset.src_pad_idx,\n                    trg_pad_idx=dataset.trg_pad_idx,\n                    trg_sos_idx=dataset.trg_sos_idx,\n                    d_model=d_model,\n                    enc_voc_size=dataset.enc_voc_size,\n                    dec_voc_size=dataset.dec_voc_size,\n                    max_len=max_len,\n                    ffn_hidden=ffn_hidden,\n                    n_head=n_heads,\n                    n_layers=n_layers,\n                    drop_prob=drop_prob,\n                    device=device).to(device)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')\nmodel.apply(initialize_weights)\noptimizer = Adam(params=model.parameters(),\n                 lr=init_lr,\n                 weight_decay=weight_decay,\n                 eps=adam_eps)\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n                                                 verbose=True,\n                                                 factor=factor,\n                                                 patience=patience)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=dataset.src_pad_idx)\n\n\ndef train(model, iterator, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    iterator = tqdm(iterator, total=len(iterator), desc='Training')\n    for i, batch in enumerate(iterator):\n        # src = batch.src\n        src,trg=batch\n        src, trg = src.to(device), trg.to(device)\n        # print(src.shape)\n\n        optimizer.zero_grad()\n        output = model(src, trg[:, :-1])\n        output_reshape = output.contiguous().view(-1, output.shape[-1])\n        trg = trg[:, 1:].contiguous().view(-1)\n\n        loss = criterion(output_reshape, trg)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n\n        epoch_loss += loss.item()\n#         print('step :', round((i / len(iterator)) * 100, 2), '% , loss :', loss.item())\n\n    return epoch_loss / len(iterator)\n\n\ndef evaluate(model, iterator, criterion):\n    model.eval()\n    epoch_loss = 0\n    batch_bleu = []\n    iterator = tqdm(iterator, total=len(iterator), desc='Evaluating')\n    with torch.no_grad():\n        for i, (src_batch, trg_batch) in enumerate(iterator):\n            src = src_batch\n            trg = trg_batch\n            src, trg = src.to(device), trg.to(device)\n            output = model(src, trg[:, :-1])\n            output_reshape = output.contiguous().view(-1, output.shape[-1])\n            output_reshape=output_reshape.to(device)\n            trg = trg[:, 1:].contiguous().view(-1)\n            trg = trg.to(output.device)\n            loss = criterion(output_reshape, trg)\n            epoch_loss += loss.item()\n\n            total_bleu = []\n            for j in range(batch_size):\n                try:\n                    trg_words = idx_to_word(batch.trg[j], loader.target.vocab)\n                    output_words = output[j].max(dim=1)[1]\n                    output_words = idx_to_word(output_words, loader.target.vocab)\n                    bleu = get_bleu(hypotheses=output_words.split(), reference=trg_words.split())\n                    total_bleu.append(bleu)\n                except:\n                    pass\n            if total_bleu:  \n                batch_bleu = sum(batch_bleu) / len(batch_bleu)\n                return epoch_loss / len(iterator), batch_bleu\n            else:\n                print(\"Warning: No BLEU scores were calculated!\")\n                return epoch_loss / len(iterator), 0.0\n\n\ndef run(total_epoch, best_loss):\n    train_losses, test_losses, bleus = [], [], []\n    for step in range(total_epoch):\n        start_time = time.time()\n        train_loss = train(model, train_iter, optimizer, criterion, clip)\n        valid_loss, bleu = evaluate(model, valid_iter, criterion)\n        end_time = time.time()\n\n        if step > warmup:\n            scheduler.step(valid_loss)\n\n        train_losses.append(train_loss)\n        test_losses.append(valid_loss)\n        bleus.append(bleu)\n        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n        if valid_loss < best_loss:\n            best_loss = valid_loss\n#             torch.save(model.state_dict(), '/kaggle/working/model-{0}.pt'.format(valid_loss))\n            \n        result_directory = '/kaggle/working/result'\n        os.makedirs(result_directory, exist_ok=True)\n        f = open('/kaggle/working/result/train_loss.txt', 'w')\n        f.write(str(train_losses))\n        f.close()\n\n        f = open('/kaggle/working/result/bleu.txt', 'w')\n        f.write(str(bleus))\n        f.close()\n\n        f = open('/kaggle/working/result/test_loss.txt', 'w')\n        f.write(str(test_losses))\n        f.close()\n\n        print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n        print(f'\\tVal Loss: {valid_loss:.3f} |  Val PPL: {math.exp(valid_loss):7.3f}')\n        print(f'\\tBLEU Score: {bleu:.3f}')\n        \n    torch.save(model.state_dict(), '/kaggle/working/model-1.pt'.format(valid_loss))\n\n\nif __name__ == '__main__':\n    run(total_epoch=epoch, best_loss=inf)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T01:55:43.194712Z","iopub.execute_input":"2024-02-17T01:55:43.195146Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"The model has 97,240,673 trainable parameters\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:36<00:00,  8.34it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 1 | Time: 1m 36s\n\tTrain Loss: 2.947 | Train PPL:  19.050\n\tVal Loss: 0.023 |  Val PPL:   1.023\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.43it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 2 | Time: 1m 35s\n\tTrain Loss: 2.445 | Train PPL:  11.526\n\tVal Loss: 0.021 |  Val PPL:   1.022\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.41it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 3 | Time: 1m 35s\n\tTrain Loss: 2.366 | Train PPL:  10.658\n\tVal Loss: 0.021 |  Val PPL:   1.021\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:34<00:00,  8.43it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 4 | Time: 1m 35s\n\tTrain Loss: 2.327 | Train PPL:  10.246\n\tVal Loss: 0.021 |  Val PPL:   1.021\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.43it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 5 | Time: 1m 35s\n\tTrain Loss: 2.293 | Train PPL:   9.901\n\tVal Loss: 0.020 |  Val PPL:   1.021\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.41it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 6 | Time: 1m 35s\n\tTrain Loss: 2.247 | Train PPL:   9.464\n\tVal Loss: 0.020 |  Val PPL:   1.020\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:34<00:00,  8.43it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 7 | Time: 1m 35s\n\tTrain Loss: 2.120 | Train PPL:   8.331\n\tVal Loss: 0.019 |  Val PPL:   1.019\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.43it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 8 | Time: 1m 35s\n\tTrain Loss: 2.076 | Train PPL:   7.974\n\tVal Loss: 0.019 |  Val PPL:   1.019\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.41it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 9 | Time: 1m 35s\n\tTrain Loss: 2.061 | Train PPL:   7.851\n\tVal Loss: 0.018 |  Val PPL:   1.019\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.42it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 10 | Time: 1m 35s\n\tTrain Loss: 2.051 | Train PPL:   7.774\n\tVal Loss: 0.018 |  Val PPL:   1.019\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.42it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 11 | Time: 1m 35s\n\tTrain Loss: 2.043 | Train PPL:   7.710\n\tVal Loss: 0.018 |  Val PPL:   1.018\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.41it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 12 | Time: 1m 35s\n\tTrain Loss: 2.031 | Train PPL:   7.622\n\tVal Loss: 0.018 |  Val PPL:   1.018\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.43it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 13 | Time: 1m 35s\n\tTrain Loss: 2.008 | Train PPL:   7.445\n\tVal Loss: 0.018 |  Val PPL:   1.018\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:34<00:00,  8.43it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 14 | Time: 1m 35s\n\tTrain Loss: 1.977 | Train PPL:   7.219\n\tVal Loss: 0.018 |  Val PPL:   1.018\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.41it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 15 | Time: 1m 35s\n\tTrain Loss: 1.952 | Train PPL:   7.040\n\tVal Loss: 0.017 |  Val PPL:   1.018\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.43it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 16 | Time: 1m 35s\n\tTrain Loss: 1.932 | Train PPL:   6.905\n\tVal Loss: 0.017 |  Val PPL:   1.017\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.42it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 17 | Time: 1m 35s\n\tTrain Loss: 1.917 | Train PPL:   6.797\n\tVal Loss: 0.017 |  Val PPL:   1.017\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.41it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 18 | Time: 1m 35s\n\tTrain Loss: 1.902 | Train PPL:   6.702\n\tVal Loss: 0.017 |  Val PPL:   1.017\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.43it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 19 | Time: 1m 35s\n\tTrain Loss: 1.888 | Train PPL:   6.607\n\tVal Loss: 0.017 |  Val PPL:   1.017\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.42it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 20 | Time: 1m 35s\n\tTrain Loss: 1.877 | Train PPL:   6.535\n\tVal Loss: 0.017 |  Val PPL:   1.017\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.41it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 21 | Time: 1m 35s\n\tTrain Loss: 1.865 | Train PPL:   6.455\n\tVal Loss: 0.017 |  Val PPL:   1.017\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.43it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 22 | Time: 1m 35s\n\tTrain Loss: 1.855 | Train PPL:   6.390\n\tVal Loss: 0.017 |  Val PPL:   1.017\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.40it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 23 | Time: 1m 35s\n\tTrain Loss: 1.846 | Train PPL:   6.332\n\tVal Loss: 0.017 |  Val PPL:   1.017\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.39it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 24 | Time: 1m 35s\n\tTrain Loss: 1.839 | Train PPL:   6.289\n\tVal Loss: 0.017 |  Val PPL:   1.017\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.41it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 25 | Time: 1m 35s\n\tTrain Loss: 1.832 | Train PPL:   6.247\n\tVal Loss: 0.016 |  Val PPL:   1.017\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.42it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 26 | Time: 1m 35s\n\tTrain Loss: 1.825 | Train PPL:   6.200\n\tVal Loss: 0.016 |  Val PPL:   1.017\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.40it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 27 | Time: 1m 35s\n\tTrain Loss: 1.819 | Train PPL:   6.169\n\tVal Loss: 0.016 |  Val PPL:   1.017\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.42it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 28 | Time: 1m 35s\n\tTrain Loss: 1.813 | Train PPL:   6.129\n\tVal Loss: 0.016 |  Val PPL:   1.016\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.42it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 29 | Time: 1m 35s\n\tTrain Loss: 1.808 | Train PPL:   6.099\n\tVal Loss: 0.016 |  Val PPL:   1.016\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.40it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 30 | Time: 1m 35s\n\tTrain Loss: 1.801 | Train PPL:   6.056\n\tVal Loss: 0.016 |  Val PPL:   1.016\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.41it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 31 | Time: 1m 35s\n\tTrain Loss: 1.796 | Train PPL:   6.027\n\tVal Loss: 0.016 |  Val PPL:   1.016\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.41it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 32 | Time: 1m 35s\n\tTrain Loss: 1.793 | Train PPL:   6.008\n\tVal Loss: 0.016 |  Val PPL:   1.016\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.40it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 33 | Time: 1m 35s\n\tTrain Loss: 1.789 | Train PPL:   5.985\n\tVal Loss: 0.016 |  Val PPL:   1.016\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.41it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 34 | Time: 1m 35s\n\tTrain Loss: 1.785 | Train PPL:   5.962\n\tVal Loss: 0.016 |  Val PPL:   1.016\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 801/801 [01:35<00:00,  8.42it/s]\nEvaluating:   0%|          | 0/100 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: No BLEU scores were calculated!\nEpoch: 35 | Time: 1m 35s\n\tTrain Loss: 1.781 | Train PPL:   5.937\n\tVal Loss: 0.016 |  Val PPL:   1.016\n\tBLEU Score: 0.000\n","output_type":"stream"},{"name":"stderr","text":"Training:  60%|█████▉    | 478/801 [00:57<00:45,  7.17it/s]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef read(name):\n    f = open(name, 'r')\n    file = f.read()\n    file = re.sub('\\\\[', '', file)\n    file = re.sub('\\\\]', '', file)\n    f.close()\n\n    return [float(i) for idx, i in enumerate(file.split(','))]\n\n\ndef draw(mode):\n    if mode == 'loss':\n        train = read('/kaggle/working/result/train_loss.txt')\n        test = read('/kaggle/working/result/test_loss.txt')\n        plt.plot(train, 'r', label='train')\n        plt.plot(test, 'b', label='validation')\n        plt.legend(loc='lower left')\n\n\n    elif mode == 'bleu':\n        bleu = read('/kaggle/working/result/bleu.txt')\n        plt.plot(bleu, 'b', label='bleu score')\n        plt.legend(loc='lower right')\n\n    plt.xlabel('epoch')\n    plt.ylabel(mode)\n    plt.title('training result')\n    plt.grid(True, which='both', axis='both')\n    plt.show()\n\n\nif __name__ == '__main__':\n    draw(mode='loss')\n    draw(mode='bleu')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T01:40:03.438458Z","iopub.status.idle":"2024-02-17T01:40:03.438797Z","shell.execute_reply.started":"2024-02-17T01:40:03.438631Z","shell.execute_reply":"2024-02-17T01:40:03.438645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_iter=DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=lambda batch: custom_collate(batch, dataset.src_sos_idx, dataset.src_eos_idx, dataset.trg_sos_idx, dataset.trg_eos_idx, dataset.src_pad_idx, dataset.trg_pad_idx))\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nmodel = Transformer(src_pad_idx=dataset.src_pad_idx,\n                    trg_pad_idx=dataset.trg_pad_idx,\n                    trg_sos_idx=dataset.trg_sos_idx,\n                    d_model=d_model,\n                    enc_voc_size=dataset.enc_voc_size,\n                    dec_voc_size=dataset.dec_voc_size,\n                    max_len=max_len,\n                    ffn_hidden=ffn_hidden,\n                    n_head=n_heads,\n                    n_layers=n_layers,\n                    drop_prob=0.00,\n                    device=device).to(device)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')\n\n\ndef test_model(num_examples):\n    iterator = test_iter\n    model.load_state_dict(torch.load(\"./saved/model-saved.pt\"))\n\n    with torch.no_grad():\n        batch_bleu = []\n        for i, batch in enumerate(iterator):\n            src = batch.src\n            trg = batch.trg\n            output = model(src, trg[:, :-1])\n\n            total_bleu = []\n            for j in range(num_examples):\n                try:\n                    src_words = idx_to_word(src[j], loader.source.vocab)\n                    trg_words = idx_to_word(trg[j], loader.target.vocab)\n                    output_words = output[j].max(dim=1)[1]\n                    output_words = idx_to_word(output_words, loader.target.vocab)\n\n                    print('source :', src_words)\n                    print('target :', trg_words)\n                    print('predicted :', output_words)\n                    print()\n                    bleu = get_bleu(hypotheses=output_words.split(), reference=trg_words.split())\n                    total_bleu.append(bleu)\n                except:\n                    pass\n\n            total_bleu = sum(total_bleu) / len(total_bleu)\n            print('BLEU SCORE = {}'.format(total_bleu))\n            batch_bleu.append(total_bleu)\n\n        batch_bleu = sum(batch_bleu) / len(batch_bleu)\n        print('TOTAL BLEU SCORE = {}'.format(batch_bleu))\n\n\nif __name__ == '__main__':\n    test_model(num_examples=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T01:40:03.440542Z","iopub.status.idle":"2024-02-17T01:40:03.440872Z","shell.execute_reply.started":"2024-02-17T01:40:03.440714Z","shell.execute_reply":"2024-02-17T01:40:03.440728Z"},"trusted":true},"execution_count":null,"outputs":[]}]}